{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Analysis with SARIMAX\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we analyze 4 years of hourly energy consumption to find trends in energy consumption around hour of the day, day of the week, season of the year, etc. and also to examine factors like outside temperature and solar installations. The goal is to build to predict the energy consumption given parameters like day of the week, time of the day, season, holiday, local weather, solar installation, etc.\n",
    "\n",
    "The energy consumption values can also be expected to depend on it’s previous lagged values because the energy consumption of a region shouldn’t be expected to change much in the next few hours except for any unexpected or unfortunate events. So we will add the lagged values of energy consumption as the X parameters and check if we can predict better using the past values (in addition to the variables that we had already added)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller,kpss\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import acf\n",
    "import statsmodels.api as sm\n",
    "from pmdarima.arima import auto_arima\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "from green_city.utils import span\n",
    "from green_city.utils import datetime2index, index2datetime\n",
    "from green_city.plotting import plot_decomposition\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "RSEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MLFLOW ##\n",
    "try:\n",
    "    import mlflow\n",
    "    from green_city.mlflow_config import get_mlflow_config\n",
    "\n",
    "    flow_conf = get_mlflow_config()\n",
    "    tracking_uri = flow_conf[\"TRACKING_URI\"]\n",
    "    mlflow.set_tracking_uri(flow_conf[\"TRACKING_URI\"])\n",
    "    mlflow.set_experiment(flow_conf[\"EXPERIMENT_NAME\"]);\n",
    "except:\n",
    "    print(\"mlflow ui not active\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## DB CONNECTION ##\n",
    "# from sqlalchemy import create_engine\n",
    "# from decouple import Config, RepositoryEnv\n",
    "\n",
    "# config = Config(RepositoryEnv(\"../.db_credentials\"))\n",
    "\n",
    "# db_connection_credentials = {\n",
    "#     \"database\": config('POSTGRES_DB'),\n",
    "#     \"user\": config('POSTGRES_USER'),\n",
    "#     \"password\": config('POSTGRES_PASSWORD'),\n",
    "#     \"host\": config('POSTGRES_HOST'),\n",
    "#     \"port\": config('POSTGRES_PORT'),\n",
    "# }\n",
    "# DB_STRING = \"postgresql://{user}:{password}@{host}:{port}/{database}\".format(**db_connection_credentials)\n",
    "# db = create_engine(DB_STRING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_nr = 5\n",
    "column_to_predict = \"net_load_kW\"\n",
    "\n",
    "#document wide parameters that don't change in experiments\n",
    "#for logging to mlflow server\n",
    "global_params = {\n",
    "    \"building nr\": building_nr,\n",
    "    \"predicted feature\": column_to_predict,\n",
    "    \"resolution\": \"daily\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(f\"../data/preprocessed/Building_{building_nr}.csv\").astype({'datetime': 'datetime64'}).set_index('datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['net_load_kW']\n",
    "df_train = df[['net_load_kW']].iloc[:(len(df)-365*24)]\n",
    "df_test = df[['net_load_kW']].iloc[(len(df)-365*24):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot train and test data\n",
    "fig, ax = plt.subplots(figsize=(12,4))\n",
    "df_train[['net_load_kW']].plot(ax=ax, label='Train data')\n",
    "df_test[['net_load_kW']].plot(ax=ax, label='Test data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_to_train = y.iloc[:(len(y)-365*24)]\n",
    "y_to_test = y.iloc[(len(y)-365*24):] # last year for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,4))\n",
    "y_to_train.plot(ax=ax)\n",
    "y_to_test.plot(ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seasonal decomposition (one year period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomp = seasonal_decompose(\n",
    "    x=y_to_train, \n",
    "    model='additive', \n",
    "    two_sided=True,\n",
    "    extrapolate_trend=True,\n",
    "    period= 24*365)\n",
    "\n",
    "plot_decomposition(decomp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple seasonal decomposition with MSTL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mstl = MSTL(y_to_train, periods=(24, 24*7, 24*365), stl_kwargs={\"seasonal_deg\": 0})\n",
    "# res = mstl.fit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Start with the plot from the results object `res`\n",
    "# plt.rc(\"figure\", figsize=(10, 14))\n",
    "# plt.rc(\"font\", size=13)\n",
    "# fig = res.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stationarity test on first difference of train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stationarity_test(y_to_train.diff().dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACF and PACF plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1, figsize=(12,8))\n",
    "fig = sm.graphics.tsa.plot_acf(y_to_train, lags=50, zero=False, ax=ax[0])\n",
    "fig = sm.graphics.tsa.plot_pacf(y_to_train, lags=50, zero=False, ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1, figsize=(12,8))\n",
    "fig = sm.graphics.tsa.plot_acf(y_to_train.diff().dropna(), lags=50, zero=False, ax=ax[0])\n",
    "fig = sm.graphics.tsa.plot_pacf(y_to_train.diff().dropna(), lags=50, zero=False, ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1, figsize=(12,8))\n",
    "fig = sm.graphics.tsa.plot_acf(y_to_train.diff().dropna().diff(24).dropna(), lags=50, zero=False, ax=ax[0])\n",
    "fig = sm.graphics.tsa.plot_pacf(y_to_train.diff().dropna().diff(24).dropna(), lags=50, zero=False, ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add fourier terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yearly, weekly and daily seasonality as exogenous variables for SARIMAX model\n",
    "exog = pd.DataFrame({'date': y.index})\n",
    "exog = exog.set_index(pd.PeriodIndex(exog['date'], freq='H'))\n",
    "exog['year_sin365'] = np.sin(2 * np.pi * exog.index.dayofyear / 365)\n",
    "exog['year_cos365'] = np.cos(2 * np.pi * exog.index.dayofyear / 365)\n",
    "exog['year_sin365_2'] = np.sin(4 * np.pi * exog.index.dayofyear / 365)\n",
    "exog['year_cos365_2'] = np.cos(4 * np.pi * exog.index.dayofyear / 365)\n",
    "exog['week_sin365'] = np.sin(2 * np.pi * exog.index.dayofweek/7)\n",
    "exog['week_cos365'] = np.cos(2 * np.pi * exog.index.dayofweek/7)\n",
    "exog['week_sin365_2'] = np.sin(4 * np.pi * exog.index.dayofweek/7)\n",
    "exog['week_cos365_2'] = np.cos(4 * np.pi * exog.index.dayofweek/7)\n",
    "exog['hour_sin365'] = np.sin(2 * np.pi * df.index.hour/24)\n",
    "exog['hour_cos365'] = np.cos(2 * np.pi * df.index.hour/24) \n",
    "exog['hour_sin365_2'] = np.sin(4 * np.pi * df.index.hour/24)\n",
    "exog['hour_cos365_2'] = np.cos(4 * np.pi * df.index.hour/24) \n",
    "exog = exog.drop(columns=['date'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto_ARIMA to determine inputs for SARIMAX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we represent the hourly, weekly and yearly as Fourier terms, we fit the auto_arima model on only the first three months of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_to_train = y.loc['2008-01-02':'2008-03-31']\n",
    "exog_to_train = exog.loc['2008-01-02':'2008-03-31']\n",
    "\n",
    "model1 = auto_arima(y=y_to_train, start_p=1, start_q=1,\n",
    "          max_p=2, max_q=2, d=1, \n",
    "          m=24, exogenous=exog_to_train, \n",
    "          seasonal=True, stationary=False,\n",
    "          information_criterion='aic',\n",
    "          stepwise=True, njobs=-1, trace=True,\n",
    "          error_action='ignore',\n",
    "          suppress_warnings=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running auto_arima, the best model that was found was:\n",
    "\n",
    "```python\n",
    "Best model:  ARIMA(6,1,1)(0,0,2)[24] intercept\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However for a similar dataset, [this notebook](https://nbviewer.org/github/pratha19/Springboard_capstone_project_1/blob/master/SDGE_energy_ML.ipynb#6.3) found the best model as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Best model:  ARIMA(1,1,2)(0,0,2)[24] intercept\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try both models for the SARIMAX model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARIMAX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARIMAX (6,1,1)(0,0,2,24) model on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model on train data\n",
    "y_to_train = y.iloc[:(len(y)-365*24)]\n",
    "y_to_test = y.iloc[(len(y)-365*24):]\n",
    "\n",
    "# Seasonality as exogenous variables\n",
    "exog_to_train = exog.iloc[:(len(y)-365*24)]\n",
    "exog_to_test = exog.iloc[(len(y)-365*24):]\n",
    "\n",
    "model_train = SARIMAX(df_train['net_load_kW'], order=(6, 1, 1), seasonal_order=(0, 0, 2, 24), exogenous=exog_to_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model_train.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = results.plot_diagnostics(figsize=(12, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = results.predict()\n",
    "y_train = df_train['net_load_kW']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,4))\n",
    "y_pred_train.rolling(24).mean().plot(ax=ax)\n",
    "y_train.rolling(24).mean().plot(ax=ax)\n",
    "\n",
    "mae = mean_absolute_error(y_train, y_pred_train)\n",
    "mse = mean_squared_error(y_train, y_pred_train)\n",
    "r2 = r2_score(y_train, y_pred_train)\n",
    "print(f\"Mean Absolute Error (SARIMAX) = {mae.round(3)}\")\n",
    "print(f\"Mean Squared Error (SARIMAX) = {mse.round(3)}\")\n",
    "print(f\"R2 score (SARIMAX) = {r2.round(3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the prediction for a few days\n",
    "fig, ax = plt.subplots(figsize=(12,4))\n",
    "y_pred_train['2008-01-02':'2008-01-06'].plot(ax=ax)\n",
    "y_train.loc['2008-01-02':'2008-01-06'].plot(ax=ax)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARIMAX (6,1,1)(0,0,2,24) forecast for 8 randomly chosen dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_to_forecast = pd.to_datetime(['2011-09-01 23:00:00', '2011-10-20 23:00:00', \n",
    "                                   '2011-01-09 06:00:00', '2011-10-22 21:00:00', \n",
    "                                   '2011-06-21 03:00:00', '2011-07-08 02:00:00', \n",
    "                                   '2011-08-19 00:00:00', '2011-04-15 07:00:00'])\n",
    "\n",
    "predictions = list()\n",
    "for day in days_to_forecast:\n",
    "  y_to_train = df['net_load_kW'].loc[day+pd.DateOffset(months=-24):day]\n",
    "  exog_to_train = exog.loc[day+pd.DateOffset(months=-24):day]\n",
    "\n",
    "  model = SARIMAX(y_to_train, order=(6, 1, 1), seasonal_order=(0, 0, 2, 24), exogenous=exog_to_train)\n",
    "  model_fit = model.fit()\n",
    "  output = model_fit.forecast(steps=24)\n",
    "  predictions.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for day_to_plot in range(len(days_to_forecast)):\n",
    "    \n",
    "    # Starting the MLFlow run\n",
    "    #r = mlflow.start_run(run_name=\"sarimax_(6,1,1)(0,0,2,24)\")\n",
    "    #print(\"run-uuid:\", r.info.run_uuid)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12,4))\n",
    "    predictions[day_to_plot].plot()\n",
    "    df_test['net_load_kW'].loc[(days_to_forecast[day_to_plot]+pd.DateOffset(hours=1)): \n",
    "                                days_to_forecast[day_to_plot]+pd.DateOffset(hours=24)].plot()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    y_pred = predictions[day_to_plot]\n",
    "    y_test = df_test['net_load_kW'].loc[(days_to_forecast[day_to_plot]+pd.DateOffset(hours=1)): \n",
    "                                days_to_forecast[day_to_plot]+pd.DateOffset(hours=24)]\n",
    "\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f\"Mean Absolute Error (SARIMAX) = {mae.round(3)}\")\n",
    "    print(f\"Mean Squared Error (SARIMAX) = {mse.round(3)}\")\n",
    "    print(f\"R2 score (SARIMAX) = {r2.round(3)}\")\n",
    "\n",
    "    ##########\n",
    "    # MLFLOW #\n",
    "    ##########\n",
    "    # for k, v in global_params.items():\n",
    "    #     mlflow.log_param(k, v)\n",
    "    #     mlflow.log_param(\"model\", \"sarimax\")\n",
    "    #     mlflow.log_param(\"datetime\", days_to_forecast[day_to_plot])\n",
    "    #     mlflow.log_param(\"feature\", \"net_load_kW\")\n",
    "    #     mlflow.log_metric(\"mse\", mse)\n",
    "    #     mlflow.log_metric(\"mae\", mae)\n",
    "    #     mlflow.log_metric(\"r2_score\", r2)\n",
    "    # mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARIMAX(2, 1, 1)(1, 0, 1, 24) model on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model on train data\n",
    "y_to_train = y.iloc[:(len(y)-365*24)]\n",
    "y_to_test = y.iloc[(len(y)-365*24):]\n",
    "\n",
    "# Seasonality as exogenous variables\n",
    "exog_to_train = exog.iloc[:(len(y)-365*24)]\n",
    "exog_to_test = exog.iloc[(len(y)-365*24):]\n",
    "\n",
    "model_train2 = SARIMAX(df_train['net_load_kW'], order=(2, 1, 1), seasonal_order=(1, 0, 1, 24), exogenous=exog_to_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results2 = model_train2.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = results2.plot_diagnostics(figsize=(12, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = results2.predict()\n",
    "y_train = df_train['net_load_kW']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,4))\n",
    "y_pred_train.rolling(24).mean().plot(ax=ax)\n",
    "y_train.rolling(24).mean().plot(ax=ax)\n",
    "\n",
    "mae = mean_absolute_error(y_train, y_pred_train)\n",
    "mse = mean_squared_error(y_train, y_pred_train)\n",
    "r2 = r2_score(y_train, y_pred_train)\n",
    "print(f\"Mean Absolute Error (SARIMAX) = {mae.round(3)}\")\n",
    "print(f\"Mean Squared Error (SARIMAX) = {mse.round(3)}\")\n",
    "print(f\"R2 score (SARIMAX) = {r2.round(3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the prediction for a few days\n",
    "fig, ax = plt.subplots(figsize=(12,4))\n",
    "y_pred_train['2008-01-02':'2008-01-06'].plot(ax=ax)\n",
    "y_train.loc['2008-01-02':'2008-01-06'].plot(ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARIMAX(2,1,1)(1,0,1,24) forecast for 8 randomly chosen dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_to_forecast = pd.to_datetime(['2011-09-01 23:00:00', '2011-10-20 23:00:00', \n",
    "                                   '2011-01-09 06:00:00', '2011-10-22 21:00:00', \n",
    "                                   '2011-06-21 03:00:00', '2011-07-08 02:00:00', \n",
    "                                   '2011-08-19 00:00:00', '2011-04-15 07:00:00'])\n",
    "\n",
    "predictions = list()\n",
    "for day in days_to_forecast:\n",
    "  y_to_train = df['net_load_kW'].loc[day+pd.DateOffset(months=-24):day]\n",
    "  exog_to_train = exog.loc[day+pd.DateOffset(months=-24):day]\n",
    "\n",
    "  model = SARIMAX(y_to_train, order=(2, 1, 1), seasonal_order=(1, 0, 1, 24), exogenous=exog_to_train)\n",
    "  model_fit = model.fit()\n",
    "  output = model_fit.forecast(steps=24)\n",
    "  predictions.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forecasts = pd.DataFrame(columns=['runid','id','prediction'])\n",
    "\n",
    "for day_to_plot in range(len(days_to_forecast)):\n",
    "\n",
    "    # Starting the MLFlow run\n",
    "    #r = mlflow.start_run(run_name=\"sarimax_(2,1,1)(1,0,1,24)\")\n",
    "    #print(\"run-uuid:\", r.info.run_uuid)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12,4))\n",
    "    predictions[day_to_plot].plot()\n",
    "    df_test['net_load_kW'].loc[(days_to_forecast[day_to_plot]+pd.DateOffset(hours=1)): \n",
    "                                days_to_forecast[day_to_plot]+pd.DateOffset(hours=24)].plot()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    y_pred = predictions[day_to_plot]\n",
    "    y_test = df_test['net_load_kW'].loc[(days_to_forecast[day_to_plot]+pd.DateOffset(hours=1)): \n",
    "                                days_to_forecast[day_to_plot]+pd.DateOffset(hours=24)]\n",
    "\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f\"Mean Absolute Error (SARIMAX) = {mae.round(3)}\")\n",
    "    print(f\"Mean Squared Error (SARIMAX) = {mse.round(3)}\")\n",
    "    print(f\"R2 score (SARIMAX) = {r2.round(3)}\")\n",
    "\n",
    "    ##########\n",
    "    # MLFLOW #\n",
    "    ##########\n",
    "    # for k, v in global_params.items():\n",
    "    #     mlflow.log_param(k, v)\n",
    "    #     mlflow.log_param(\"model\", \"sarimax\")\n",
    "    #     mlflow.log_param(\"datetime\", days_to_forecast[day_to_plot])\n",
    "    #     mlflow.log_param(\"feature\", \"net_load_kW\")\n",
    "    #     mlflow.log_metric(\"mse\", mse)\n",
    "    #     mlflow.log_metric(\"mae\", mae)\n",
    "    #     mlflow.log_metric(\"r2_score\", r2)\n",
    "    # mlflow.end_run()\n",
    "\n",
    "    ##########\n",
    "    # SQL DB #\n",
    "    ##########\n",
    "    # start_date = days_to_forecast[day_to_plot]+pd.DateOffset(hours=1)\n",
    "    # end_date = days_to_forecast[day_to_plot]+pd.DateOffset(hours=24)\n",
    "    # predict_dates = pd.date_range(start=start_date, end=end_date, freq='H')\n",
    "\n",
    "    # forecast = pd.DataFrame(columns=['runid','id','prediction'])\n",
    "    # forecast['prediction'] = y_pred\n",
    "    # forecast['runid'] = r.info.run_uuid\n",
    "    # forecast['id'] = np.array(datetime2index(predict_dates))\n",
    "    # forecasts = pd.concat([forecasts, forecast], axis=0)\n",
    "\n",
    "#forecasts.to_csv('sarimax_(2,1,1)(1,0,1,24).csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.8 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "adec6c5f7a7e6b374924807d676c9e580fd19dbf1c8cad640c4c5a1bb48bcada"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
